= Rate Limiting Policy
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]


*Eva, I don't see any option for Throttling policy in the UI. Do we need to keep rate limiting and throttling together or can we separate it. I suggest keeping things as they are displayed in the UI.*

The Rate Limiting policy limits the number of requests an API can accept within a specified time window. The API rejects any requests that exceed the limit. You can configure multiple limits with window sizes ranging from milliseconds to years.

// The Throttling policy queues requests that exceed limits for possible processing in a subsequent window. The API eventually rejects the request if processing fails after a certain number of attempts. You can configure a delay between retries, as well as limit the number of retries. -- applies to Spike Control

The Rate Limiting policies impose a limit on all requests or a specific resource (in Mule 3.x and earlier, the API must be APIkit-based). The service level access (SLA)-based Rate Limiting and Throttling policies add further granularity, limiting requests by the level of access granted to the requesting application. These policies contain a persistence engine to preserve the current state of the policy in case of sudden restart (power outage).

// == Terminology

// The following terms apply to rate limiting and throttling:

// * Limits: A series of pairs--quota, window
// * Quota: The `# of Reqs` you configure in API Manager
// * Window: API Manager pair--time period, time unit
// * Retries: Throttling capability to queue requests
// * Contracts: relationship between an Application and an SLA


== How this Policy Works

When you specify a request limit in API Manager, the _quota_ per _time window_ configuration for a rate limiting and throttling algorithm is created. This algorithm is created on demand, when the first request is received. This event fixes the time window.

The policy creates one algorithm for each limit with its _quota_ per _time window_ configuration. Therefore, when multiple limits are configured, every algorithm must have available quota in its current window for the request to be accepted.

Each request consumes a quota of tries from the current window until the time expires. When the quota is exhausted, the resulting action depends on the policy that is applied (*is it applied or does it automatically get triggered based on a flag?--Eva)*

* Rate limiting policy rejects the request
* Throttling policy queues the request for retry

When the time window closes, the quota is reset and a new window of the same fixed size is initiated. 

== Configuring the Policy

=== Response Headers

The following access-limiting policies return headers with details about the current state of the request:

* X-Ratelimit-Remaining: The amount of available quota.
* X-Ratelimit-Limit: The maximum available requests per window.
* X-Ratelimit-Reset: The remaining time, in milliseconds, until a new window starts.

=== Throttling Retries

Because throttling is intended to smooth spikes, Mule Runtime can delay requests and retry the requests later. The following examples describe how requests are rejected or accepted.

=== Rejected Request

Policy Configuration: 5 requests/10 seconds window, 1 retry with 500ms delay

image::throttling-rejected-request.png[]

Users send 5 requests within the first six seconds, and all are accepted. At 8 seconds, a new request comes in. No quota remains, so the request is delayed for 500ms.

At the 8.5 seconds, the queued request is reprocessed. As the window has not yet closed, there is still no quota available. Also, there are no more retries available, so the request is rejected.

=== Accepted Request

Policy Configuration: 5 requests/10 seconds window, 1 retry with 500ms delay

image::throttling-accepted-request.png[]

Users send 5 requests within the first nine seconds, all of them are accepted.
At 9.7 seconds, a new request comes in. No quota remains, so the request is delayed for 500ms.

At 10 seconds, the window closes, and quota is reset to 5.
At 10.2 seconds since the start (the 0.2 second mark of the current window) the queued request is reprocessed. Now, there is available quota, so the request is accepted (available quota decreases to 4).

=== Configuration

Because the Throttling policy works over the HTTP stack, an open connection must be preserved between the user and the API for each of the queued requests to be reprocessed. This process is resource intensive, and on large windows, HTTP or the underlying TCP may cancel the connection due to timeout.

Although not enforced in Gateway 2.x and Mule Runtime 3.x, small windows are recommended. A one (1) second maximum window is considered small. Using this policy in a standalone environment is recommended.

== Cluster vs. Standalone

One way to achieve resiliency is to use redundancy: multiple Mule Runtime instances serving the same API.

A consideration in this situation is whether or not rate limiting (with or without SLA) should be shared. You can set up Mule Runtimes to run in a cluster to share the configured limits and treat all nodes as a whole. You can configure the rate limiting policy to share the quota among the cluster nodes or not. By default, the quota is shared among the cluster nodes.

The following analyses cover different scenarios.

=== Decentralized processing

_n_ nodes where:

* Each has its own backend.
* The fastest response is required.
* The number of served requests are limited only by the backend capacity.
* SLA levels are _not_ available or each node serves only one.
+
For example, you need two nodes; one is assigned to the billing users of the company, and the other is assigned to the rest.

For decentralized processing in this scenario, there is no need for cluster. Just set the policy below the backend capacity of the weakest of the nodes. In the no SLA scenario, a load balancer could be useful in case a node goes down.

=== Centralized processing

_n_ nodes where:

* The backend is centralized.
* The number of served requests are limited only by the backend capacity.
* SLA levels are _not_ available or every node serves the same, single SLA.
+
For example, every node is assigned to the billing users of the company.

If there is a load balancer up front, there is no need for a cluster. If _q_ is the maximum capacity of the backend, then configure the policy as follows:

image::quota-policy-formula.png[]

where image:omega.png[] is a small number, below the backend maximum capacity.

If there is no load balancer, cluster mode is recommended over standalone as you cannot configure beforehand how much traffic each node will handle. These policies are designed to work both on a perfectly balanced workload, or completely uneven. The backend will not receive any extra requests.

=== Multiple Workers

_n_ CloudHub workers where:

* Each one represents the same API.
* The application workload is equally distributed across the workers.

The approach should be the same as the _Centralized processing_ use case.

=== Using Several Contracts

_n_ nodes where:

SLAs are applied.

If a Rate Limiting SLA is applied, and each node must accept requests from multiple SLAs, then a cluster is a good choice in this situation, as you cannot determine beforehand how many requests from each SLA each node will serve.

=== Time Window Sizes in a Cluster

In a cluster, the nodes must share information for consistency across the cluster. The sharing process adds latency that must be taken into account when reviewing performance.

In the worst case scenario, the number of penalized requests with latency due to cluster consistency is constant and independent from the actual size of the configured quota. Consequently, the smaller the window, the greater the percentage of potentially delayed requests. Therefore, MuleSoft strongly recommends setting _only_ window sizes greater than one minute in Rate Limiting and Rate Limiting SLA policy configurations.

=== Minimizing Latency

The clustered algorithm minimizes the amount of shared information to maximize performance. Response headers (the X-Rate-Limit headers) are calculated with a heuristic that predicts the size of available quota in the cluster, without the need for resynchronization on every request. The error in the headers information is always less than 10%. Still, the amount of accepted requests will not be greater than the defined quota.

=== Configuration

In API Gateway Runtime 2.x and Mule Runtime 3.x, Rate Limiting policies with and without SLA will automatically run distributed in a cluster. You cannot turn off this feature.

== Persistence

You can configure rate limiting and throttling algorithms to use big windows sizes: days, months, years. For example, suppose as a client you want to allow your user X to consume 1M requests per year. You cannot predict whether the node will be up the entire period or need maintenance, which may result in restarting the runtime. The algorithm has been running for several months, so the client will lose critical information. Persistence solves this problem by periodically saving the current policy state. In case of a redeployment or restart, the algorithms are recreated from the last known persisted state or started from a clean state.

Although persistence is enabled by default, you can turn it off by setting the following property to false:

`throttling.persistence_enabled`

You can also tweak the persistence frequency rate, which has a default of 10 seconds:

`throttling.persistent_data_update_freq`

*IMPORTANT:* This feature is disabled on CloudHub.

== See Also

* xref:tutorial-manage-an-api.adoc[Applying a Policy and SLA Tier]
* xref:delete-sla-tier-task.adoc[Removing SLA Tiers on API Manager]
* xref:resource-level-policies-about.adoc[Reviewing Resource Level Policies]